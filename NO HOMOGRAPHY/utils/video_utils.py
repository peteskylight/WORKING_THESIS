import cv2
from PySide6.QtCore import QThread, Signal
import numpy as np

from ultralytics import YOLO
import queue
import threading
from PySide6.QtGui import QImage, QPixmap

from pathlib import Path

class VideoUtils:
    
    def __init__(self) -> None:
        pass

    def read_video(self, video_path, resize_frames): #LIST
        cap = cv2.VideoCapture(video_path)
        frames = []
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            if resize_frames:
                resized_frame = cv2.resize(frame, (1088, 608))
                frames.append(resized_frame)
            else:
                frames.append(frame)
        cap.release()
        return frames
    #Old Code
    
    def save_video(self, output_video_frames, output_video_path, monitorFrames=False):
        fourcc = cv2.VideoWriter_fourcc(*'MJPG')
        
        out = cv2.VideoWriter(output_video_path, fourcc, 24, (output_video_frames[0].shape[1], output_video_frames[0].shape[0]))
        
        for frame in output_video_frames:
            out.write(frame)
            if monitorFrames:
                cv2.imshow("Monitor Frames", frame)
                cv2.waitKey(10)
        out.release()
    
    def generate_white_frame(self, height, width):
        white_frame = np.ones((height, width, 3), dtype=np.uint8) * 255 # Create a white frame (all pixel values set to 255)
        return white_frame

class WhiteFrameGenerator(QThread):
    progress_update = Signal(object)
    return_white_frames = Signal(object)
    
    def __init__(self, number_of_frames, width, height):
        super().__init__()
        self.number_of_frames = number_of_frames
        self.videoWidth = width
        self.videoHeight = height
        self._running = True
    def run(self):
        white_frames = []
        current_frame = 0
        total_frames_length = self.number_of_frames
        for frame in range(total_frames_length):
            white_frame = np.ones((self.videoHeight, self.videoWidth, 3), dtype=np.uint8) * 0 
            white_frames.append(white_frame)
            current_frame += 1
            progress = int(current_frame/total_frames_length *100)
            
            self.progress_update.emit(progress)
            
            del white_frame
            del progress
            
        self.return_white_frames.emit(white_frames)
        del white_frames
    def stop(self):
        self._running = False
        self.wait()


class VideoProcessorThread(QThread):
    """
    This class is used to process the video in a thread.
    It will process the video frame by frame and emit signals with the results.
    What I mean with processing is to detect humans and their keypoints frame by frame. 
    Appending each result in the corresponding list.
    Emmits signals with the results.

    Here is the pseudo code:
    1. Read the video
    2. Get the total frames
    3. Loop through the video frames
       (Inside the loop)
        4. Get the frame
        5. Detect humans in the frame
        6. Append the results in the human_detect_results_list
        7. Detect the keypoints in the frame based on the results of the cropped image in the human_detect_results_list
        8. Append the results in the human_pose_results_list
        9. Update the progress
    (Outside the loop)
    10. Emit the signals with the results

    """
    #Initiate the signals

    human_detect_results = Signal(object)
    human_pose_results = Signal(object)
    progress_update = Signal(object)

    def __init__(self, video_path,
                 resize_frames = False,
                 isFront = True,
                 human_detection_model = None,
                 human_detection_confidence = 0.5,
                 human_pose_model = None,
                 human_pose_confidence = 0.5,
                 main_window = None):

        super().__init__()

        self.isFront = isFront
        self.human_detection_model = YOLO(human_detection_model)  
        self.human_detection_confidence = human_detection_confidence
        self.human_pose_model = YOLO(human_pose_model)
        self.human_pose_confidence = human_pose_confidence
        self.main_window = main_window

        self.human_detect_results_list = []
        self.human_pose_results_list = []

        self.video_path = video_path
        self.resize_frames = resize_frames

        self.initial_row_height = None

        self._running = True

    #Mask for front camera
    def create_roi_mask(self, frame, row_height):
        height, width, _ = frame.shape
        mask = np.zeros((height, width), dtype=np.uint8)
        roi = (0, row_height+10, width, height)  # Bottom to middle
        cv2.rectangle(mask, (roi[0], roi[1]), (roi[2], roi[3]), 255, -1)
        cv2.line(img=frame, pt1=(0, row_height), pt2=(frame.shape[1], row_height),color = (0,255,0), thickness=2)
        return mask
    
    #Human Detection
    def human_detect(self, frame):

        #Just to declare the variables
        id_name_dict = None
        student_dict = None
        results = None
        #Add conditional to create mask or not
        if self.isFront:
            roi_mask = self.create_roi_mask(frame, self.initial_row_height)
            masked_frame = cv2.bitwise_and(frame, frame, mask=roi_mask)
            results = self.human_detection_model.track(masked_frame,
                                                conf = self.human_detection_confidence,
                                                persist=True,
                                                classes=0,
                                                iou=0.3,
                                                agnostic_nms=True)[0]
            id_name_dict = results.names
            student_dict = {}
        else:
            results = self.human_detection_model.track(frame,
                                                conf = self.human_detection_confidence,
                                                persist=True,
                                                classes=0,
                                                iou=0.3,
                                                agnostic_nms=True)[0]
            id_name_dict = results.names
            student_dict = {}

        for result in results:
            boxes = result.boxes
            for box in boxes:
                #Get the image per person
                b = box.xyxy[0]
                c = box.cls
                
                if box.id is not None and box.xyxy is not None and box.cls is not None:
                    track_id = int(box.id.tolist()[0])
                    track_result = box.xyxy.tolist()[0]
                    object_cls_id = box.cls.tolist()[0]
                    object_cls_name = id_name_dict.get(object_cls_id, "unknown")
                    if object_cls_name == "person":
                        student_dict[track_id] = track_result
                else:
                    print("One of the attributes is None:", box.id, box.xyxy, box.cls)
                    student_dict[track_id] = None
        return student_dict
    
    #Human Pose Detection
    def human_pose_detect(self, frame, human_results):
        keypoints_dict = {}  # Initialize here to store keypoints for all detections in the frame

        for track_id, bbox in human_results.items():
            # Extract the bounding box coordinates
            x1, y1, x2, y2 = map(int, bbox)
            cropped_image = frame[y1:y2, x1:x2]

            try:
                # Perform pose detection on the cropped image
                results = self.human_pose_model(cropped_image, self.human_pose_confidence)
                for result in results:
                    if result.keypoints:
                        keypoints_normalized = np.array(result.keypoints.xyn.cpu().numpy()[0])
                        keypoints_dict[track_id] = keypoints_normalized
                    else:
                        print(f"Error processing track ID {track_id}: {e}")
            except Exception as e:
                print(f"Error processing track ID {track_id}: {e}")
                keypoints_dict[track_id] = None
        
        return keypoints_dict

    #Main function to run in a thread
    def run(self):
        #Get the video
        cap = cv2.VideoCapture(self.video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        current_frame = 0

        #Variables for the human detection model
        
        while self._running:
            ret, frame = cap.read() #Get the frame

            #Safety net if there is no frame anymore
            if not ret:
                break

            #Get the shape of the frame
            height, width, _ = frame.shape

            if self.resize_frames:
                frame = cv2.resize(frame, (1088, 608))

            #Get the initial row height
            self.initial_row_height = int(height * (1/16))  # Bottom 4 rows (adjust as needed)
            
            human_detect_results = self.human_detect(frame)
            human_pose_detect_results = self.human_pose_detect(frame, human_detect_results)

            self.human_detect_results_list.append(human_detect_results)
            self.human_pose_results_list.append(human_pose_detect_results)

            current_frame += 1
            progress = int(current_frame/total_frames *100)
            self.progress_update.emit(progress)
            
            del frame #Just to delete unnecessary data

        self.human_detect_results.emit(self.human_detect_results_list)
        self.human_pose_results.emit(self.human_pose_results_list)

        #Just to delete unnecessary data
        del self.human_detect_results_list
        del self.human_pose_results_list

        cap.release() #Release the video

    def stop(self):
        self._running = False
        self.wait()

############################################################################################################


class VideoPlayerThread(QThread):
    '''
    The technique used in playing the video is to preload the frames in a queue and then play them in a loop.
    This is done to ensure that the video is played smoothly without any lagging.
    The technique is called "Frame Buffering WITH Lazy Lodaing
    Producer-Consumer Pattern (Using queue.Queue)
        Producer: The preload_frames() function continuously reads frames in the background and stores them in a queue (queue.Queue).
        Consumer: The run() method fetches preloaded frames from the queue and sends them to the UI for display.
    Advantage: This prevents the UI thread from waiting for frame decoding, making playback smooth.
         Lazy Loading (On-Demand Processing)

    Instead of storing all frames in RAM, we load only the next few frames (maxsize=10 in the queue).
    When a frame is needed, it's already decoded and ready for display.

    '''

    
    frames_signal = Signal(object)
    #cv2.setNumThreads(1)  # Disable OpenCV multi-threading to reduce CPU usage

    def __init__(self, center_video_path, front_video_path, main_window):
        super().__init__()

        self.main_window = main_window

        self.center_video_path = center_video_path
        self.front_video_path = front_video_path

        #Separate captures for each video

        self.center_cap = cv2.VideoCapture(self.center_video_path)
        self.front_cap = cv2.VideoCapture(self.front_video_path)

        #For all
        self.running = True
        self.paused = False

        self.max_size = 30
        #Keep preloaded frames here

        self.center_video_frame_queue = queue.Queue(maxsize=self.max_size)  
        self.center_video_black_frame_queue = queue.Queue(maxsize=self.max_size) 
        self.front_video_frame_queue = queue.Queue(maxsize=self.max_size)  
        self.front_video_black_frame_queue = queue.Queue(maxsize=self.max_size)  

        self.current_center_video_frame_index = 0
        self.current_front_video_frame_index = 0
        self.current_frame_index = 0

        self.target_frame_index = 0 # Set target frame index
        self.preload_thread = threading.Thread(target=self.preload_frames, daemon=True)
        self.preload_thread.start()  # Start background frame loader

        self.skeleton_pairs = [
            (0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 6), 
            (5, 7), (6, 8), (7, 9), (8, 10), (5, 11), (6, 12), (11, 12), 
            (11, 13), (12, 14), (13, 15), (14, 16)
        ]


    def write_action_text(self, frame, black_frame, detections, actions):
        """
        Draw bounding boxes with action labels on each frame.
        """

        for person_id, bbox in detections.items():
            x, y, w, h = map(int, bbox)  # Ensure coordinates are integers
            action_text = f"Action: {actions.get(person_id, 'Unknown')}"
            
            # Ensure text position is within frame bounds
            text_x, text_y = x, max(y - 10, 10)  # Prevent negative y
            
            #cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
            cv2.putText(frame, action_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(black_frame, action_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        return frame, black_frame

    def drawing_bounding_box(self, video_frame, results):

        black_frame = np.zeros((1080, 1920, 3), dtype=np.uint8)
        item_count = 0

        
        
        for track_id, bbox in results.items():
            x1, y1, x2, y2 = bbox
            cv2.putText(video_frame, f"Student ID: {track_id}", (int(bbox[0]), int(bbox[1] - 20)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)
            cv2.rectangle(video_frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)

            cv2.putText(black_frame, f"Student ID: {track_id}", (int(bbox[0]), int(bbox[1] - 20)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)
            cv2.rectangle(black_frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
        
        return video_frame, black_frame
    
    def drawing_keypoints(self, keypoints_dict, detection, black_frame, video_frame):
        for track_id in keypoints_dict:
            if track_id in detection:
                keypoints = keypoints_dict[track_id]
                bbox = detection[track_id]
                bbox_x, bbox_y, bbox_w, bbox_h = bbox

                cropped_frame = black_frame[int(bbox_y):int(bbox_h), int(bbox_x):int(bbox_w)]
                video_cropped_frame = video_frame[int(bbox_y):int(bbox_h), int(bbox_x):int(bbox_w)]

                for keypoint in keypoints:
                    x = int(keypoint[0] * cropped_frame.shape[1])
                    y = int(keypoint[1] * cropped_frame.shape[0])
                    cv2.circle(cropped_frame, (x, y), radius=4, color=(0, 255, 0), thickness=-1)
                    cv2.circle(video_cropped_frame, (x, y), radius=4, color=(0, 255, 0), thickness=-1)

                # Draw skeleton
                for pair in self.skeleton_pairs:
                    if pair[0] < len(keypoints) and pair[1] < len(keypoints):
                        pt1 = keypoints[pair[0]]
                        pt2 = keypoints[pair[1]]
                        #print(f"pt1: {pt1}, pt2: {pt2}")
                        # # Debug statement
                        if isinstance(pt1, np.ndarray) and isinstance(pt2, np.ndarray):
                            x1 = int(pt1[0] * cropped_frame.shape[1])
                            y1 = int(pt1[1] * cropped_frame.shape[0])
                            x2 = int(pt2[0] * cropped_frame.shape[1])
                            y2 = int(pt2[1] * cropped_frame.shape[0])
                            
                            if 0.1 <= x1 < cropped_frame.shape[1] and 0.1 <= y1 < cropped_frame.shape[0] and 0.1 <= x2 < cropped_frame.shape[1] and 0.1 <= y2 < cropped_frame.shape[0]:
                                cv2.line(cropped_frame, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)
                                cv2.line(video_cropped_frame, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)
                        else:
                            print(f"Invalid keypoints for track ID {track_id}: pt1={pt1}, pt2={pt2}")
                    else:
                        print(f"Keypoints array too small for track ID {track_id}: {keypoints}")

                # Place the cropped frame back into the original frame

                black_frame[int(bbox_y):int(bbox_h), int(bbox_x):int(bbox_w)] = cropped_frame
                video_frame[int(bbox_y):int(bbox_h), int(bbox_x):int(bbox_w)] = video_cropped_frame
        
        return video_frame, black_frame

    def preload_frames(self):
        results = None
        keypoints = None
        center_video_black_frame = None
        front_video_black_frame = None

        front_video_results = self.main_window.human_detect_results_front
        front_video_keypoints = self.main_window.human_pose_results_front
        front_video_actions = self.main_window.action_results_list_front

        center_video_results = self.main_window.human_detect_results_center
        center_video_keypoints = self.main_window.human_pose_results_center
        center_video_actions = self.main_window.action_results_list_center

        """
        Background thread to keep decoding frames ahead of playback
        """

        while self.running:
            if not self.paused and (not self.center_video_frame_queue.full()):
                front_ret, front_frame = self.front_cap.read()
                center_ret, center_frame = self.center_cap.read()

                if (not front_ret) or (not center_ret):
                    self.center_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Restart video if it ends
                    self.front_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    self.current_frame_index = 0
                    self.target_frame_index = 0 
                    continue
                
                # Draw bounding boxes and keypoints only for the current frame index
                '''
                LOL EQUAL LANG NAMAN EH! NAGLOLOKOHAN LANG TAYO RITO AHAHHAHHAHA
                '''

                if self.current_frame_index == self.target_frame_index:  # Draw only for target frame index #
                    center_frame, center_video_black_frame = self.drawing_bounding_box(video_frame=center_frame,
                                                                results=center_video_results[int(self.current_frame_index)])
                    
                    center_frame, center_video_black_frame = self.drawing_keypoints(keypoints_dict=center_video_keypoints[int(self.current_frame_index)],
                                                                detection=center_video_results[int(self.current_frame_index)],
                                                                black_frame=center_video_black_frame,
                                                                video_frame=center_frame)
                    center_frame, center_video_black_frame = self.write_action_text(frame=center_frame,
                                                                                    black_frame=center_video_black_frame,
                                                                                    detections=center_video_results[int(self.current_frame_index)],
                                                                                    actions=center_video_actions[int(self.current_frame_index)])

                    
                    front_frame, front_video_black_frame = self.drawing_bounding_box(video_frame=front_frame,
                                                                results=front_video_results[int(self.current_frame_index)])
                    
                    front_frame, front_video_black_frame = self.drawing_keypoints(keypoints_dict=front_video_keypoints[int(self.current_frame_index)],
                                                                detection=front_video_results[int(self.current_frame_index)],
                                                                black_frame=front_video_black_frame,
                                                                video_frame=front_frame)
                    
                    front_frame, front_video_black_frame = self.write_action_text(frame=front_frame,
                                                                                  black_frame=front_video_black_frame,
                                                                                  detections=front_video_results[int(self.current_frame_index)],
                                                                                  actions=front_video_actions[int(self.current_frame_index)])

                else:
                    # If not the target frame, skip drawing
                    center_frame = center_frame  # Just pass the frame unchanged
                    center_video_black_frame = center_video_black_frame  # Just pass the black_frame unchanged
                    front_frame = front_frame
                    front_video_black_frame = front_video_black_frame
                
                # Store the frame with the drawn bounding box and keypoints
                if center_frame is not None and center_video_black_frame is not None:
                    self.center_video_frame_queue.put(center_frame)
                    self.center_video_black_frame_queue.put(center_video_black_frame)

                if front_frame is not None and front_video_black_frame is not None:
                    self.front_video_frame_queue.put(front_frame)
                    self.front_video_black_frame_queue.put(front_video_black_frame)


                self.current_frame_index += 1
                self.target_frame_index +=1

        # self.center_cap.release()
        # self.front_cap.release()

    def run(self):
        """Main playback loop, sending frames from queue to UI"""
        while self.running:
            if not self.paused and not self.center_video_frame_queue.empty():
                center_video_frame = self.center_video_frame_queue.get()  # Get frame from queue
                center_black_frame = self.center_video_black_frame_queue.get()

                front_video_frame = self.front_video_frame_queue.get()  # Get frame from queue
                front_black_frame = self.front_video_black_frame_queue.get()

                center_frame = None
                front_frame = None

                front_frame = front_video_frame
                center_frame = center_video_frame
            

                # height, width, channels = frame.shape
                # bytes_per_line = channels * width
                # qimg = QImage(frame.data, width, height, bytes_per_line, QImage.Format_RGB888)
                self.frames_signal.emit([center_frame, front_frame])  # Send frame to UI

                cv2.waitKey(1000//30)  # Play at ~30 FPS


    def stop(self):
        """Stops video playback and terminates threads."""
        self.running = False  # Stop preload loop
        self.quit()
        self.wait()
        if self.preload_thread.is_alive():
            self.preload_thread.join()  # Ensure the thread exits cleanly

        self.center_cap.release()
        self.front_cap.release()


    def pause(self, status):
        self.paused = status
        if status:
            self.main_window.play_pause_button_video_preview.setText("PLAY PREVIEW")
        else:
            self.main_window.play_pause_button_video_preview.setText("PAUSE PREVIEW")


class SeekingVideoPlayerThread(QThread):
    '''
    The technique used in playing the video is to preload the frames in a queue and then play them in a loop.
    This is done to ensure that the video is played smoothly without any lagging.
    The technique is called "Frame Buffering WITH Lazy Lodaing
    Producer-Consumer Pattern (Using queue.Queue)
        Producer: The preload_frames() function continuously reads frames in the background and stores them in a queue (queue.Queue).
        Consumer: The run() method fetches preloaded frames from the queue and sends them to the UI for display.
    Advantage: This prevents the UI thread from waiting for frame decoding, making playback smooth.
         Lazy Loading (On-Demand Processing)

    Instead of storing all frames in RAM, we load only the next few frames (maxsize=10 in the queue).
    When a frame is needed, it's already decoded and ready for display.

    '''

    
    frames_signal = Signal(object)
    #cv2.setNumThreads(1)  # Disable OpenCV multi-threading to reduce CPU usage

    def __init__(self, center_video_path, front_video_path, main_window):
        super().__init__()

        self.main_window = main_window

        self.center_video_path = center_video_path
        self.front_video_path = front_video_path

        #Separate captures for each video

        self.center_cap = cv2.VideoCapture(self.center_video_path)
        self.front_cap = cv2.VideoCapture(self.front_video_path)

        #For all
        self.running = True
        self.paused = False

        self.max_size = 30


        #Keep preloaded frames here

        self.classroom_heatmap_frames_queue = queue.Queue(maxsize=self.max_size)

        self.original_center_video_frame_queue = queue.Queue(maxsize=self.max_size)  
        self.center_video_frame_queue = queue.Queue(maxsize=self.max_size)  
        self.center_video_black_frame_queue = queue.Queue(maxsize=self.max_size) 

        self.original_front_video_frame_queue = queue.Queue(maxsize=self.max_size)  
        self.front_video_frame_queue = queue.Queue(maxsize=self.max_size)  
        self.front_video_black_frame_queue = queue.Queue(maxsize=self.max_size)  

        self.current_center_video_frame_index = 0
        self.current_front_video_frame_index = 0
        self.current_frame_index = 0

        self.target_frame_index = 0 # Set target frame index
        self.preload_thread = threading.Thread(target=self.preload_frames, daemon=True)
        self.preload_thread.start()  # Start background frame loader

        #For getting the seatplan image hehe
        script_dir = Path(__file__).parent  # Get script's folder
        image_path = script_dir.parent / "assets" / "SEAT PLAN.png"
        self.seat_plan_picture = cv2.imread(image_path)
        self.seat_plan_picture_previous_frame = None
        self.isFirstFrame =True

        self.skeleton_pairs = [
            (0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 6), 
            (5, 7), (6, 8), (7, 9), (8, 10), (5, 11), (6, 12), (11, 12), 
            (11, 13), (12, 14), (13, 15), (14, 16)
        ]


    def write_action_text(self, frame, black_frame, detections, actions):
        """
        Draw bounding boxes with action labels on each frame.
        """

        for person_id, bbox in detections.items():
            x, y, w, h = map(int, bbox)  # Ensure coordinates are integers
            action_text = f"Action: {actions.get(person_id, 'Unknown')}"
            
            # Ensure text position is within frame bounds
            text_x, text_y = x, max(y - 10, 10)  # Prevent negative y
            
            #cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
            cv2.putText(frame, action_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(black_frame, action_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        return frame, black_frame

    def drawing_bounding_box(self, video_frame, results):

        black_frame = np.zeros((1080, 1920, 3), dtype=np.uint8)
        item_count = 0

        
        
        for track_id, bbox in results.items():
            x1, y1, x2, y2 = bbox
            cv2.putText(video_frame, f"Student ID: {track_id}", (int(bbox[0]), int(bbox[1] - 20)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)
            cv2.rectangle(video_frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)

            cv2.putText(black_frame, f"Student ID: {track_id}", (int(bbox[0]), int(bbox[1] - 20)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)
            cv2.rectangle(black_frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
        
        return video_frame, black_frame
    
    def drawing_keypoints(self, keypoints_dict, detection, black_frame, video_frame):
        for track_id in keypoints_dict:
            if track_id in detection:
                keypoints = keypoints_dict[track_id]
                bbox = detection[track_id]
                bbox_x, bbox_y, bbox_w, bbox_h = bbox

                cropped_frame = black_frame[int(bbox_y):int(bbox_h), int(bbox_x):int(bbox_w)]
                video_cropped_frame = video_frame[int(bbox_y):int(bbox_h), int(bbox_x):int(bbox_w)]

                for keypoint in keypoints:
                    x = int(keypoint[0] * cropped_frame.shape[1])
                    y = int(keypoint[1] * cropped_frame.shape[0])
                    cv2.circle(cropped_frame, (x, y), radius=4, color=(0, 255, 0), thickness=-1)
                    cv2.circle(video_cropped_frame, (x, y), radius=4, color=(0, 255, 0), thickness=-1)

                # Draw skeleton
                for pair in self.skeleton_pairs:
                    if pair[0] < len(keypoints) and pair[1] < len(keypoints):
                        pt1 = keypoints[pair[0]]
                        pt2 = keypoints[pair[1]]
                        #print(f"pt1: {pt1}, pt2: {pt2}")
                        # # Debug statement
                        if isinstance(pt1, np.ndarray) and isinstance(pt2, np.ndarray):
                            x1 = int(pt1[0] * cropped_frame.shape[1])
                            y1 = int(pt1[1] * cropped_frame.shape[0])
                            x2 = int(pt2[0] * cropped_frame.shape[1])
                            y2 = int(pt2[1] * cropped_frame.shape[0])
                            
                            if 0.1 <= x1 < cropped_frame.shape[1] and 0.1 <= y1 < cropped_frame.shape[0] and 0.1 <= x2 < cropped_frame.shape[1] and 0.1 <= y2 < cropped_frame.shape[0]:
                                cv2.line(cropped_frame, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)
                                cv2.line(video_cropped_frame, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)
                        else:
                            print(f"Invalid keypoints for track ID {track_id}: pt1={pt1}, pt2={pt2}")
                    else:
                        print(f"Keypoints array too small for track ID {track_id}: {keypoints}")

                # Place the cropped frame back into the original frame

                black_frame[int(bbox_y):int(bbox_h), int(bbox_x):int(bbox_w)] = cropped_frame
                video_frame[int(bbox_y):int(bbox_h), int(bbox_x):int(bbox_w)] = video_cropped_frame
        
        return video_frame, black_frame
    
    def drawing_classroom_heatmap(self, frame, results):
        # Use a cached heatmap to avoid recomputing everything
        if self.isFirstFrame:
            self.heatmap_image = self.seat_plan_picture.copy()
            self.isFirstFrame = False
        else:
            self.heatmap_image = frame.copy()

        # Process people in parallel
        radius = 150
        gradient_circle = self.create_gradient_circle(radius, (0, 0, 255), 16)  # Precompute once

        for person_id, bbox in results.items():
            center = (int((bbox[0] + bbox[2]) / 2), int((bbox[1] + bbox[3]) / 2))
            self.overlay_image_alpha(self.heatmap_image, gradient_circle, (center[0] - radius, center[1] - radius))

        self.seat_plan_picture_previous_frame = self.heatmap_image
        return self.heatmap_image


        # Function to create a gradient circle
    def create_gradient_circle(self, radius, color, max_alpha):
        Y, X = np.ogrid[:2*radius, :2*radius]
        center = radius
        dist_from_center = np.sqrt((X - center) ** 2 + (Y - center) ** 2)

        alpha = np.clip(max_alpha - (max_alpha * (dist_from_center / radius)), 0, max_alpha).astype(np.uint8)

        # Vectorized assignment
        gradient_circle = np.zeros((2*radius, 2*radius, 4), dtype=np.uint8)
        gradient_circle[..., :3] = color  # Set RGB channels
        gradient_circle[..., 3] = alpha  # Set alpha channel

        return gradient_circle



    # Function to overlay image with alpha
    def overlay_image_alpha(self, img, img_overlay, pos):
        x, y = pos
        h, w = img_overlay.shape[:2]

        # Compute overlap regions
        y1, y2 = max(0, y), min(img.shape[0], y + h)
        x1, x2 = max(0, x), min(img.shape[1], x + w)

        y1o, y2o = max(0, -y), min(h, img.shape[0] - y)
        x1o, x2o = max(0, -x), min(w, img.shape[1] - x)

        if y1 >= y2 or x1 >= x2 or y1o >= y2o or x1o >= x2o:
            return

        # Vectorized alpha blending
        img_overlay_crop = img_overlay[y1o:y2o, x1o:x2o]
        alpha = img_overlay_crop[..., 3:4] / 255.0  # Keep alpha as (h, w, 1) for broadcasting
        img[y1:y2, x1:x2, :3] = (alpha * img_overlay_crop[..., :3] +
                                (1 - alpha) * img[y1:y2, x1:x2, :3]).astype(np.uint8)




    def preload_frames(self):
        results = None
        keypoints = None
        center_video_black_frame = None
        front_video_black_frame = None
        front_classroom_heatmap = None
        center_classroom_heatmap = None

        front_video_results = self.main_window.human_detect_results_front
        front_video_keypoints = self.main_window.human_pose_results_front
        front_video_actions = self.main_window.action_results_list_front

        center_video_results = self.main_window.human_detect_results_center
        center_video_keypoints = self.main_window.human_pose_results_center
        center_video_actions = self.main_window.action_results_list_center

        """
        Background thread to keep decoding frames ahead of playback
        """

        while self.running:
            if not self.paused and (not self.center_video_frame_queue.full()):
                front_ret, front_frame = self.front_cap.read()
                center_ret, center_frame = self.center_cap.read()


                
 
                original_front_frame = front_frame.copy()
                original_center_frame = center_frame.copy()
                
                min_value, max_value = self.main_window.timeFrameRangeSlider.value()

                #Make sure that the videos are starting based on the set range sliders
                if self.isFirstFrame:
                    self.center_cap.set(cv2.CAP_PROP_POS_FRAMES, min_value)
                    self.front_cap.set(cv2.CAP_PROP_POS_FRAMES, min_value)
                    self.isFirstFrame = False
                    self.seat_plan_picture_previous_frame = self.seat_plan_picture.copy()

                if (not front_ret) or (self.current_frame_index > (max_value-1)):
                    self.center_cap.set(cv2.CAP_PROP_POS_FRAMES, min_value)  # Restart video if it ends
                    self.front_cap.set(cv2.CAP_PROP_POS_FRAMES, min_value)

                    self.seat_plan_picture_previous_frame = self.seat_plan_picture.copy()
                    self.isFirstFrame = True

                    self.current_frame_index = min_value
                    self.target_frame_index = min_value
                    continue
                
                # Draw bounding boxes and keypoints only for the current frame index
                '''
                LOL EQUAL LANG NAMAN EH! NAGLOLOKOHAN LANG TAYO RITO AHAHHAHHAHA
                '''

                if self.current_frame_index == self.target_frame_index:  # Draw only for target frame index #
                    #DEBUGGING
                    print("CURRENT INDEX: ", self.current_center_video_frame_index)
                    print("LENGTH OF LIST", len(center_video_results))
                    center_frame, center_video_black_frame = self.drawing_bounding_box(video_frame=center_frame,
                                                                results=center_video_results[int(self.current_frame_index)])
                    
                    center_frame, center_video_black_frame = self.drawing_keypoints(keypoints_dict=center_video_keypoints[int(self.current_frame_index)],
                                                                detection=center_video_results[int(self.current_frame_index)],
                                                                black_frame=center_video_black_frame,
                                                                video_frame=center_frame)
                    center_frame, center_video_black_frame = self.write_action_text(frame=center_frame,
                                                                                    black_frame=center_video_black_frame,
                                                                                    detections=center_video_results[int(self.current_frame_index)],
                                                                                    actions=center_video_actions[int(self.current_frame_index)])

                    
                    front_frame, front_video_black_frame = self.drawing_bounding_box(video_frame=front_frame,
                                                                results=front_video_results[int(self.current_frame_index)])
                    
                    front_frame, front_video_black_frame = self.drawing_keypoints(keypoints_dict=front_video_keypoints[int(self.current_frame_index)],
                                                                detection=front_video_results[int(self.current_frame_index)],
                                                                black_frame=front_video_black_frame,
                                                                video_frame=front_frame)
                    
                    front_frame, front_video_black_frame = self.write_action_text(frame=front_frame,
                                                                                  black_frame=front_video_black_frame,
                                                                                  detections=front_video_results[int(self.current_frame_index)],
                                                                                  actions=front_video_actions[int(self.current_frame_index)])
                    heatmap_frame_threshold = 30
                    if self.current_frame_index % heatmap_frame_threshold == 0:
                        #For heatmap
                        front_classroom_heatmap = self.drawing_classroom_heatmap(frame=self.seat_plan_picture_previous_frame,
                                                                                results=front_video_results[int(self.current_frame_index)])
                        
                        #For center and whole heatmap
                        center_classroom_heatmap = self.drawing_classroom_heatmap(frame=front_classroom_heatmap,
                                                                                results=center_video_results[int(self.current_frame_index)])

                        #For heatmap (Can be placed anywhere)
                        self.classroom_heatmap_frames_queue.put(center_classroom_heatmap)
                    else:
                        if self.isFirstFrame:
                            self.classroom_heatmap_frames_queue.put(self.seat_plan_picture)
                        else:
                            self.classroom_heatmap_frames_queue.put(self.seat_plan_picture_previous_frame)

                else:
                    # If not the target frame, skip drawing
                    self.center_cap.set(cv2.CAP_PROP_POS_FRAMES, self.current_frame_index)
                    self.front_cap.set(cv2.CAP_PROP_POS_FRAMES, self.current_frame_index)
                    self.current_frame_index = self.target_frame_index
                
                # Store the frame with the drawn bounding box and keypoints
                if center_frame is not None and center_video_black_frame is not None:
                    self.original_center_video_frame_queue.put(original_center_frame)
                    self.center_video_frame_queue.put(center_frame)
                    self.center_video_black_frame_queue.put(center_video_black_frame)

                    


                if front_frame is not None and front_video_black_frame is not None:
                    self.original_front_video_frame_queue.put(original_front_frame)
                    self.front_video_frame_queue.put(front_frame)
                    self.front_video_black_frame_queue.put(front_video_black_frame)

                
                self.current_frame_index += 1
                self.target_frame_index +=1

        # self.center_cap.release()
        # self.front_cap.release()

    def run(self):
        """Main playback loop, sending frames from queue to UI"""
        while self.running:
            if not self.paused and not self.center_video_frame_queue.empty():
                center_video_frame = self.center_video_frame_queue.get()  # Get frame from queue
                center_black_frame = self.center_video_black_frame_queue.get()
                original_center_frame = self.original_center_video_frame_queue.get()

                front_video_frame = self.front_video_frame_queue.get()  # Get frame from queue
                front_black_frame = self.front_video_black_frame_queue.get()
                original_front_frame = self.original_front_video_frame_queue.get()

                classroom_heatmap = self.classroom_heatmap_frames_queue.get()

                # if self.current_frame_index == self.target_frame_index:  
                #     self.frames_signal.emit([center_video_frame, front_video_frame])

                center_frame = None
                front_frame = None

                if self.main_window.keypointsOnlyChkBox_front_analytics.isChecked():
                    self.main_window.originalVideoOnlyChkBox_front_analytics.setEnabled(False)
                    front_frame = front_black_frame
                else:
                    self.main_window.originalVideoOnlyChkBox_front_analytics.setEnabled(True)
                    front_frame = front_video_frame

                if self.main_window.keypointsOnlyChkBox_Center_analytics.isChecked():
                    self.main_window.originalVideoOnlyChkBox_center_analytics.setEnabled(False)
                    center_frame = center_black_frame
                else:
                    self.main_window.originalVideoOnlyChkBox_center_analytics.setEnabled(True)
                    center_frame = center_video_frame

                if self.main_window.originalVideoOnlyChkBox_front_analytics.isChecked():
                    self.main_window.keypointsOnlyChkBox_front_analytics.setEnabled(False)
                    front_frame = original_front_frame
                else:
                    self.main_window.keypointsOnlyChkBox_front_analytics.setEnabled(True)
                    front_frame = front_video_frame
                
                if self.main_window.originalVideoOnlyChkBox_center_analytics.isChecked():
                    self.main_window.keypointsOnlyChkBox_Center_analytics.setEnabled(False)
                    center_frame = original_center_frame
                else:
                    self.main_window.keypointsOnlyChkBox_Center_analytics.setEnabled(True)
                    center_frame = center_video_frame

                self.frames_signal.emit([center_frame, front_frame, classroom_heatmap])  # Send frameS to UI displayer/updater

                
                
                cv2.waitKey(1000//30)  # Play at ~30 FPS

    def stop(self):
        self.running = False
        self.quit()
        self.wait()

    def pause(self, status):
        self.paused = status
        if status:
            self.main_window.play_pause_button_video_preview.setText("PLAY PREVIEW")
        else:
            self.main_window.play_pause_button_video_preview.setText("PAUSE PREVIEW")


    def seek_frame(self, frame_index):
        """Seeks to a specific frame in the video when the slider is moved."""
        self.paused = True  # Pause while seeking

        # Update the frame index
        self.current_frame_index = frame_index
        self.target_frame_index = frame_index

        # Move the video capture to the requested frame
        self.center_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
        self.front_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)

        # Clear the frame queues to load new frames from the seek position
        with self.center_video_frame_queue.mutex:
            self.center_video_frame_queue.queue.clear()
        with self.front_video_frame_queue.mutex:
            self.front_video_frame_queue.queue.clear()

        with self.center_video_black_frame_queue.mutex:
            self.center_video_black_frame_queue.queue.clear()
        with self.front_video_black_frame_queue.mutex:
            self.front_video_black_frame_queue.queue.clear()

        self.paused = False  # Resume after seeking
